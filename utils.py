import math
import os
from io import StringIO
import tempfile
import json
import random
import requests
import streamlit as st
import pandas as pd
import geopandas as gpd
import geemap
import shapely
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
import matplotlib.pyplot as plt
import numpy as np
import sys
import ee

@st.cache_resource
def initialize_gee():
    try:
        service_account_info = dict(st.secrets["earthengine"])
        with tempfile.NamedTemporaryFile(mode='w+', suffix='.json', delete=False) as f:
            json.dump(service_account_info, f)
            f.flush()
            credentials = ee.ServiceAccountCredentials(service_account_info["client_email"], f.name)
            ee.Initialize(credentials, project=credentials.project_id)
            st.success("GEE initialization success.")
    except Exception as e:
        st.error("Error when intializing Google Earth Engine. Verify credentials in st.secrets.")
        st.error(f"Error details: {e}")
        st.stop()

def mask_s2_clouds(image):
  """Masks clouds in a Sentinel-2 image using the QA band.

  Args:
      image (ee.Image): A Sentinel-2 image.

  Returns:
      ee.Image: A cloud-masked Sentinel-2 image.
  """
  qa = image.select('QA60')

  # Bits 10 and 11 are clouds and cirrus, respectively.
  cloud_bit_mask = 1 << 10
  cirrus_bit_mask = 1 << 11

  # Both flags should be set to zero, indicating clear conditions.
  mask = (
      qa.bitwiseAnd(cloud_bit_mask)
      .eq(0)
      .And(qa.bitwiseAnd(cirrus_bit_mask).eq(0))
  )

  return image.updateMask(mask).divide(10000)


# Calculate Index

#Nari

def calculate_nari(img):
    green = img.select('B3')
    red_edge_1 = img.select('B5')
    nari = ((ee.Image.constant(1).divide(green)).subtract(ee.Image.constant(1).divide(red_edge_1))).divide((ee.Image.constant(1).divide(green)).add(ee.Image.constant(1).divide(red_edge_1)))
    return nari

# Ncri

def calculate_ncri(img):
    red_edge_1 = img.select('B5')
    red_edge_3 = img.select('B7')
    ncri = ((ee.Image.constant(1).divide(red_edge_1)).subtract(ee.Image.constant(1).divide(red_edge_3))).divide((ee.Image.constant(1).divide(red_edge_1)).add(ee.Image.constant(1).divide(red_edge_3)))
    return ncri

def gee_calculate_scrub_index(index: str=None, year: int=None):
    sentinel = (ee.ImageCollection("COPERNICUS/S2_SR_HARMONIZED")
                .filterDate(f'{year}-09-01', f'{year}-11-01')
                .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 65))
                .map(mask_s2_clouds)
                .select(['B3', 'B5', 'B7'])
    )
    if index =='nari':
        nari = sentinel.map(calculate_nari)
        return nari.median()
    elif index == 'ncri':
        ncri = sentinel.map(calculate_ncri)
        return ncri.median()
    else:
        print('ERROR. No layer returned')
        
def calculate_ndvi(year: int=None):
    sentinel = (ee.ImageCollection("COPERNICUS/S2_SR_HARMONIZED")
                .filterDate(f'{year}-01-01', f'{year+1}-01-01')
                .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 65))
                .map(mask_s2_clouds)
                .select(['B3', 'B5', 'B7'])
    )
    red = sentinel.select('B4')
    nir = sentinel.select('B8')
    ndvi = (red.subtract(nir)).divide(red.add(nir)).mean()
    return ndvi       

def get_era5_snowfall(date, poi):
    window_end = ee.Date(date)
    window_start = window_end.advance(ee.Number(-4), 'days')
    era5 = (
            ee.ImageCollection('ECMWF/ERA5_LAND/DAILY_AGGR')
            .filterDate(window_start, window_end)
            .select('snowfall_sum')
            .sum()
            .multiply(1000)
        )
    return era5.sampleRegions(
        collection = poi,
        properties = ['name', 'datetime']
    )

def get_era5_snowcover(date, poi):
    window_end = ee.Date(date)
    window_start = window_end.advance(ee.Number(-4), 'days')
    era5 = (
            ee.ImageCollection('ECMWF/ERA5_LAND/DAILY_AGGR')
            .filterDate(window_start, window_end)
            .select('snow_cover')
            .mean()
        )
    return era5.sampleRegions(
        collection = poi,
        properties = ['name', 'datetime']
)
@st.cache_data
def get_species_data(species_name, country_code, database = 'gbif'):
    """
    Retrieves observational data for a specific species using the GBIF API and returns it as a pandas DataFrame.

    Parameters:
    species_name (str): The scientific name of the species to query.
    country_code (str): The country code of the where the observation data will be queried.

    Returns:
    pd.DataFrame: A pandas DataFrame containing the observational data.
    """
    if(database == 'iNaturalist'): user_id = input('Type in User-ID: ')
    
    match database:
        case 'gbif': base_url = "https://api.gbif.org/v1/occurrence/search"
        case 'iNaturalist': base_url = f"https://api.inaturalist.org/v1/observations?user_id={user_id}&quality_grade=needs_id&rank=genus"
    params = {
        "scientificName": species_name,
        "country": country_code,
        "hasCoordinate": "true",
        "basisOfRecord": "HUMAN_OBSERVATION",
        "limit": 10000,
    }

    try:
        response = requests.get(base_url, params=params)
        response.raise_for_status()  # Raises an exception for a response error.
        data = response.json()
        occurrences = data.get("results", [])

        if occurrences:  # If data is present
            df = pd.json_normalize(occurrences)
            return gpd.GeoDataFrame(
                df,
                geometry=gpd.points_from_xy(df.decimalLongitude, df.decimalLatitude), crs="EPSG:4326")[["species", "year", "month", "geometry"]]
        else:
            print("No data found for the given species and country code.")
            return None  # Returns an empty DataFrame
    except requests.RequestException as e:
        print(f"Request failed: {e}")
        return None  # Returns an empty DataFrame in case of an exception
    
def remove_duplicates(data, grain_size, aoi):
    # Select one occurrence record per pixel at the chosen spatial resolution
    random_raster = ee.Image.random().reproject("EPSG:4326", None, grain_size).clip(aoi)
    rand_point_vals = random_raster.sampleRegions(
        collection=ee.FeatureCollection(data), geometries=True
    )
    return rand_point_vals.distinct("random")

def get_aoi_from_nuts(country_code:str = "AT", county_name:str=None):
    NUTS_0 = gpd.read_file(r"./assets/NUTS_RG_01M_2024_4326_LEVL_0.geojson")
    NUTS_2 = gpd.read_file(r"./assets/NUTS_RG_01M_2024_4326_LEVL_2.geojson")
    country = geemap.gdf_to_ee(NUTS_0.loc[NUTS_0.CNTR_CODE==country_code])
    if county_name:
        county = geemap.gdf_to_ee(NUTS_2.loc[NUTS_2.NUTS_NAME==county_name])  
    else:
        county = None
    
    return country, county

def get_layer_information(year: int):
    era5 = (
            ee.ImageCollection('ECMWF/ERA5_LAND/HOURLY')
            .filter(ee.Filter.date(f"{year}-01-01", f"{year+1}-01-01"))
            )
    BIOCLIM = {key: ee.Image("projects/ee-sebasd1991/assets/BioClim").select(key) for key in ee.Image("projects/ee-sebasd1991/assets/BioClim").bandNames().getInfo()}
    terrain = ee.Algorithms.Terrain(ee.Image("USGS/SRTMGL1_003"))
    canopyHeight = ee.ImageCollection("projects/sat-io/open-datasets/facebook/meta-canopy-height").mosaic()
    layer= {
            "landcover" : ee.Image("COPERNICUS/CORINE/V20/100m/2018").select('landcover'),
            "elevation" : ee.Image("USGS/SRTMGL1_003").select('elevation'),
            "slope" : terrain.select('slope'),
            "aspect" : terrain.select('aspect'),
            "northness" : terrain.select('aspect').multiply(math.pi/180).cos().rename('northness'),
            "eastness" : terrain.select('aspect').multiply(math.pi/180).sin().rename('eastness'),
            "NDVI" : (
                    ee.ImageCollection('LANDSAT/COMPOSITES/C02/T1_L2_8DAY_NDVI')
                    .filterDate(f"{year}-01-01", f"{year+1}-01-01")
                    .mean()
                ),
            "GHMI" : ee.ImageCollection("projects/sat-io/open-datasets/GHM/HM_2022_300M").first().rename('GHMI'),
            "Trees" : canopyHeight.updateMask(canopyHeight.gte(1)).rename('Trees'),
            "SWE" : era5.select(['snow_depth_water_equivalent'], ['swe']).mean().rename('SWE'),
            "snow_depth" : era5.select(['snow_depth'], ['snow_depth']).mean(),
            "snow_cover" : era5.select(['snow_cover'], ['snow_cover']).mean(),
            "snow_albedo" : era5.select(['snow_albedo'], ['snow_albedo']).mean(),
            "CHM" : canopyHeight.rename('CHM'),
            "NARI" : gee_calculate_scrub_index('nari', year).rename('NARI'),
            "NCRI" : gee_calculate_scrub_index('ncri', year).rename('NCRI')
        }
    return {**layer, **BIOCLIM}

def get_layer_visualization_params(layer_name: str):
    try:
        paletteHM = ['#4c6100','#adda25','#e2ff9b','#ffff73','#ffe629','#ffd37f','#ffaa00','#e69808','#e60000','#a80000','#730000']
        paletteTrees = ["#4A2354", '#fde725']
        vis_params = {
            "elevation": {"min": 0, "max": 4000, "palette": geemap.colormaps.palettes['terrain']},
            "slope": {"min": 0, "max": 60, "palette": geemap.colormaps.palettes['viridis']},
            "NDVI": {"min": -1, "max": 1, "palette": geemap.colormaps.palettes['RdYlGn']},
            "CHM": {"min": 0, "max": 25, "palette": geemap.colormaps.palettes['viridis']},
            "Trees": {"min": 1, "max": 1, "palette": paletteTrees},
            "NARI": {"min": -1, "max": 1, "palette": geemap.colormaps.palettes['RdYlGn']},
            "NCRI": {"min": -1, "max": 1, "palette": geemap.colormaps.palettes['RdYlGn']},
            "GHMI": {"min": 0, "max": 1, "palette": paletteHM},
            "SWE": {"min": 0, "max": 500, "palette": geemap.colormaps.palettes['Blues']},
            "snow_depth": {"min": 0, "max": 200, "palette": geemap.colormaps.palettes['Blues']},
            "snow_cover": {"min": 0, "max": 100, "palette": geemap.colormaps.palettes['Blues']},
            "snow_albedo": {"min": 0, "max": 1, "palette": geemap.colormaps.palettes['Greys']},
            "b1": {"min": -30, "max": 50, "palette": geemap.colormaps.palettes['viridis']},
            "aspect": {"min": 0, "max": 360, "palette": geemap.colormaps.palettes['hsv']},
            "northness": {"min": -1, "max": 1, "palette": geemap.colormaps.palettes['coolwarm']},
            "eastness": {"min": -1, "max": 1, "palette": geemap.colormaps.palettes['coolwarm']}
        }
        return vis_params.get(layer_name, {})
    except:
        return {}

def plot_correlation_heatmap(dataframe, h_size=10, show_labels=False):
    # Calculate Spearman correlation coefficients
    correlation_matrix = dataframe.corr(method="spearman")

    # Create a heatmap
    plt.figure(figsize=(h_size, h_size-2))
    plt.imshow(correlation_matrix, cmap='coolwarm', interpolation='nearest')

    # Optionally display values on the heatmap
    if show_labels:
        for i in range(correlation_matrix.shape[0]):
            for j in range(correlation_matrix.shape[1]):
                plt.text(j, i, f"{correlation_matrix.iloc[i, j]:.2f}",
                         ha='center', va='center', color='white', fontsize=8)

    columns = dataframe.columns.tolist()
    plt.xticks(range(len(columns)), columns, rotation=90)
    plt.yticks(range(len(columns)), columns)
    plt.title("Variables Correlation Matrix")
    plt.colorbar(label="Spearman Correlation")
    plt.savefig('correlation_heatmap_plot.png')
    plt.show()

@st.cache_data
def load_background_data():
    bg_df = pd.read_csv(r"./assets/background_data.csv", sep=',')
    s = bg_df['.geo'].astype(str).str.replace("'", '"').apply(json.loads)
    geoms = s.apply(shapely.geometry.shape)
    bg_gdf = gpd.GeoDataFrame(bg_df.drop(['.geo'], axis=1), geometry=geoms, crs='epsg:4326')
    
    return bg_gdf
    
    
def compute_sdm(species_gdf: gpd.GeoDataFrame=None, features: list=None, prediction_aoi: ee.Geometry=None, model_type: str="Random Forest", n_trees: int=100, tree_depth: int=5, train_size: float=0.7, year: int=2024):
    from sklearn.metrics import r2_score, roc_auc_score
    from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, StratifiedShuffleSplit, cross_val_score, GridSearchCV
    from sklearn.feature_selection import RFE, RFECV, SelectFromModel
    
    if model_type== "Maxent":
        background_gdf = load_background_data()[features+['geometry']]
    else:
        background_gdf = load_background_data()[features+['geometry']]
    layer = get_layer_information(year)
    
    predictors = ee.Image.cat([layer[feature] for feature in features])
    presence_gdf = geemap.ee_to_gdf(predictors.sampleRegions(collection=geemap.gdf_to_ee(species_gdf), geometries=True))
    
    background_gdf['PresAbs'] = 0
    presence_gdf['PresAbs'] = 1   
    
    presence_gdf = presence_gdf[background_gdf.columns]

    ml_gdf = pd.concat([background_gdf, presence_gdf], axis=0).reset_index(drop=True)
    ml_gdf.columns = [_.replace(' ','_') for _ in ml_gdf.columns]

    y = ml_gdf['PresAbs']
    X = ml_gdf[features]
   
    match model_type:
        case "Random Forest":
            results = []

            for i in range(10):
                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=train_size, stratify=y, shuffle=True)
                model = RandomForestClassifier(n_estimators=n_trees, max_samples=0.8, min_samples_leaf=.1, verbose=0, class_weight='balanced', max_depth=tree_depth)
                model.fit(X_train, y_train)
                results.append([roc_auc_score(y_test, model.predict_proba(X_test)[:,1])] + model.feature_importances_.tolist())
                
                results_df = pd.DataFrame(results, columns=['roc_auc'] + X.columns.tolist())
        case "Maxent":
            model = "Maxent"
            results_df = pd.DataFrame()
    #partial dependence
    # from sklearn.inspection import PartialDependenceDisplay, permutation_importance

    # # fig, ax = plt.subplots(figsize=(12, 8))
    # _, ax = plt.subplots(ncols=4, nrows=2, figsize=(9, 8), constrained_layout=True)
    # ax = ax.flatten()[:-1]  # remove the last subplot

    # PartialDependenceDisplay.from_estimator(
    #     rf, X_train, features=X.columns.to_list(), kind='average', ax=ax
    # )
    return model, results_df, ml_gdf, predictors

def classify_image_aoi(image, aoi, ml_gdf, model, features):
    if "classified_img_pr" in st.session_state:
        del st.session_state.classified_img_pr
    fc = geemap.gdf_to_ee(ml_gdf)
    seed=random.randint(1,1000)
    tr_presence_points = fc.filter(ee.Filter.eq('PresAbs', 1)).randomColumn(seed=seed).sort("random")
    tr_pseudo_abs_points = fc.filter(ee.Filter.eq('PresAbs', 0)).randomColumn(seed=seed).sort("random").limit(tr_presence_points.size().getInfo())
    train_pvals = tr_presence_points.merge(tr_pseudo_abs_points)
    if isinstance(model, RandomForestClassifier):
        # Random Forest classifier
        classifier = ee.Classifier.smileRandomForest(
            seed=seed,
            numberOfTrees=model.n_estimators,
            maxNodes=model.max_depth,
            # shrinkage=0.1, # gradient
            # variablesPerSplit=None,
            minLeafPopulation=round(train_pvals.size().getInfo()*.1, 0),#rf
            bagFraction=0.8, # rf
        )
        classifier_pr = classifier.setOutputMode("PROBABILITY").train(
            train_pvals, "PresAbs", features
        )
        classified_img_pr = image.clip(aoi).classify(classifier_pr)
        return classified_img_pr
       
    else:
        classifier = ee.Classifier.amnhMaxent()
    
        # Presence probability: Habitat suitability map
        classifier_pr = classifier.train(
            train_pvals, "PresAbs", features
        )
        classified_img_pr = image.clip(aoi).classify(classifier_pr)
        return classified_img_pr.select('probability')
    
    
def plot_hier_clustering(dataframe):
    from scipy.cluster import hierarchy
    from scipy.spatial.distance import squareform
    from scipy.stats import spearmanr
    from collections import defaultdict
    
    X=dataframe.copy().drop('geometry', axis=1)
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))
    corr = spearmanr(X).correlation

    # Ensure the correlation matrix is symmetric
    corr = (corr + corr.T) / 2
    np.fill_diagonal(corr, 1)

    # We convert the correlation matrix to a distance matrix before performing
    # hierarchical clustering using Ward's linkage.
    distance_matrix = 1 - np.abs(corr)
    dist_linkage = hierarchy.ward(squareform(distance_matrix))
    dendro = hierarchy.dendrogram(
        dist_linkage, labels=X.columns.to_list(), ax=ax1, leaf_rotation=90
    )
    dendro_idx = np.arange(0, len(dendro["ivl"]))

    ax2.imshow(corr[dendro["leaves"], :][:, dendro["leaves"]])
    ax2.set_xticks(dendro_idx)
    ax2.set_yticks(dendro_idx)
    ax2.set_xticklabels(dendro["ivl"], rotation="vertical")
    ax2.set_yticklabels(dendro["ivl"])
    _ = fig.tight_layout()
    
    cluster_ids = hierarchy.fcluster(dist_linkage, .35, criterion="distance")
    cluster_id_to_feature_ids = defaultdict(list)
    for idx, cluster_id in enumerate(cluster_ids):
        cluster_id_to_feature_ids[cluster_id].append(idx)
    selected_features = [v[0] for v in cluster_id_to_feature_ids.values()]
    selected_features_names = X.columns[selected_features]
    selected_features_names
    
    
    return fig, selected_features_names